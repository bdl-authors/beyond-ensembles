---
# Slurm config
name: "SLURM"
partition: "gpu_4"
job-name: "cifar_10"
num_parallel_jobs: 120
ntasks: 1
cpus-per-task: 8
mem-per-cpu: 4000
time: 60
sbatch_args:
  gres: "gpu:1"
  # container-image: ./../../../bdl_container.sqsh
  # container-mounts: ./../..:/work,/etc/slurm/task_prolog:/etc/slurm/task_prolog
  # container-workdir: /work/experiments/civilcomments
  # container-writable: ""

---
name: "DEFAULT"
path: "results"
repetitions: 1
reps_per_job: 1
reps_in_parallel: 1

# Train: 50000 datapoints

params:
  batch_size: 128
  data_path: "../../data/"
  use_amp: False
  eval_samples: 10
  ece_bins: 10
  disable_wandb: False
  seed_offset: 0
  eval_batch_size: 512
  share_file_system: False
  lr_schedule: True

---
name: "HMC"
params:
  model: "hmc"
