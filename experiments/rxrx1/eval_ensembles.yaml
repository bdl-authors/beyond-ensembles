---
# Slurm config
name: "SLURM"
partition: "gpu_4"
job-name: "rxrx1"
num_parallel_jobs: 120
ntasks: 1
cpus-per-task: 8
mem-per-cpu: 4000
time: 600
sbatch_args:
  gres: "gpu:1"
  # container-image: ./../../../bdl_container.sqsh
  # container-mounts: ./../..:/work,/etc/slurm/task_prolog:/etc/slurm/task_prolog
  # container-workdir: /work/experiments/civilcomments
  # container-writable: ""
sh_lines:
  - "mkdir -p $TMP/data/wilds"
  - "tar -C $TMP/data/wilds/ -xvzf $(ws_find data-ssd)/data/wilds/rxrx1_v1.0.tgz"

---
name: "DEFAULT"
path: "results"
repetitions: 5
reps_per_job: 1
reps_in_parallel: 1

# Train: 129809 datapoints
# Test (OOD): 42791 datapoints

params:
  batch_size: 75
  # data_path: "$TMP/data/"
  data_path: "../../data/"
  use_amp: True
  eval_samples: 10
  ece_bins: 10
  eval_while_train: True
  disable_wandb: False
  subsample: 0
  test_subsample: 0
  static_bn: True
  seed_offset: 0

# lr and weight decay values are based on the results in https://worksheets.codalab.org/worksheets/0x036017edb3c74b0692831fadfe8cbf1b (WILDS original)

---
name: "DeepEnsemble"
params:
  model: "map"
  run_name: "MAP"
  members: 5
  seed_offset: 0
  base_optimizer:
    lr: 0.0001
    weight_decay: 0.00001

---
name: "MultiMCD"
params:
  model: "mcd"
  run_name: "MCD_p_0.1"
  members: 5
  ll_dropout_p: 0.1
  base_optimizer:
    lr: 0.0001
    weight_decay: 0.00001

---
name: "MultiSWAG"
params:
  model: "swag"
  run_name: "SWAG"
  members: 5
  base_optimizer:
    lr: 0.0001
    weight_decay: 0.00001
  swag:
    update_interval: 630 # 35 * 540 / 30
    start_epoch: 40
    deviation_samples: 30
